// ---------------------------------------------------------
// This file was generated by parol.
// It is not intended for manual editing and changes will be
// lost after next build.
// ---------------------------------------------------------

use parol_runtime::once_cell::sync::Lazy;
#[allow(unused_imports)]
use parol_runtime::parser::{LLKParser, LookaheadDFA, ParseTreeType, ParseType, Production, Trans};
use parol_runtime::{ParolError, ParseTree, TerminalIndex};
use parol_runtime::{TokenStream, Tokenizer};
use std::path::Path;

use crate::parser::sdc_grammar::SdcGrammar;
use crate::parser::sdc_grammar_trait::SdcGrammarAuto;

use parol_runtime::lexer::tokenizer::{ERROR_TOKEN, UNMATCHABLE_TOKEN, WHITESPACE_TOKEN};

pub const TERMINALS: &[&str; 15] = &[
    /*  0 */ UNMATCHABLE_TOKEN,
    /*  1 */ UNMATCHABLE_TOKEN,
    /*  2 */ UNMATCHABLE_TOKEN,
    /*  3 */ UNMATCHABLE_TOKEN,
    /*  4 */ UNMATCHABLE_TOKEN,
    /*  5 */ r"\[",
    /*  6 */ r"\]",
    /*  7 */ r"\{[^}]*\}",
    /*  8 */ r"\u{0022}(?:\\[\u{0022}\\/bfnrt]|u[0-9a-fA-F]{4}|[^\u{0022}\\])*\u{0022}",
    /*  9 */ r"#.*(\r\n|\r|\n|$)",
    /* 10 */ r";",
    /* 11 */ r"\\(\r\n|\r|\n)",
    /* 12 */ r"(\r\n|\r|\n|$)",
    /* 13 */ r"[^\s\[\]]+",
    /* 14 */ ERROR_TOKEN,
];

pub const TERMINAL_NAMES: &[&str; 15] = &[
    /*  0 */ "EndOfInput",
    /*  1 */ "Newline",
    /*  2 */ "Whitespace",
    /*  3 */ "LineComment",
    /*  4 */ "BlockComment",
    /*  5 */ "TermLBracket",
    /*  6 */ "TermRBracket",
    /*  7 */ "TermBraceGroup",
    /*  8 */ "TermStringGroup",
    /*  9 */ "TermComment",
    /* 10 */ "TermSemiColon",
    /* 11 */ "TermBackslashLineBreak",
    /* 12 */ "TermLineBreak",
    /* 13 */ "TermWord",
    /* 14 */ "Error",
];

/* SCANNER_0: "INITIAL" */
const SCANNER_0: (&[&str; 5], &[TerminalIndex; 9]) = (
    &[
        /*  0 */ UNMATCHABLE_TOKEN,
        /*  1 */ UNMATCHABLE_TOKEN,
        /*  2 */ WHITESPACE_TOKEN,
        /*  3 */ UNMATCHABLE_TOKEN,
        /*  4 */ UNMATCHABLE_TOKEN,
    ],
    &[
        5,  /* TermLBracket */
        6,  /* TermRBracket */
        7,  /* TermBraceGroup */
        8,  /* TermStringGroup */
        9,  /* TermComment */
        10, /* TermSemiColon */
        11, /* TermBackslashLineBreak */
        12, /* TermLineBreak */
        13, /* TermWord */
    ],
);

const MAX_K: usize = 1;

pub const NON_TERMINALS: &[&str; 28] = &[
    /*  0 */ "Argument",
    /*  1 */ "Command",
    /*  2 */ "CommandLine",
    /*  3 */ "CommandList",
    /*  4 */ "CommandReplacement",
    /*  5 */ "Source",
    /*  6 */ "SourceList",
    /*  7 */ "SourceListGroup",
    /*  8 */ "TermBackslashLineBreak",
    /*  9 */ "TermBraceGroup",
    /* 10 */ "TermComment",
    /* 11 */ "TermLBracket",
    /* 12 */ "TermLineBreak",
    /* 13 */ "TermRBracket",
    /* 14 */ "TermSemiColon",
    /* 15 */ "TermStringGroup",
    /* 16 */ "TermWord",
    /* 17 */ "TokenBraceGroup",
    /* 18 */ "TokenBraceGroupOpt",
    /* 19 */ "TokenEnd",
    /* 20 */ "TokenLBracket",
    /* 21 */ "TokenLBracketOpt",
    /* 22 */ "TokenRBracket",
    /* 23 */ "TokenRBracketOpt",
    /* 24 */ "TokenStringGroup",
    /* 25 */ "TokenStringGroupOpt",
    /* 26 */ "TokenWord",
    /* 27 */ "TokenWordOpt",
];

pub const LOOKAHEAD_AUTOMATA: &[LookaheadDFA; 28] = &[
    /* 0 - "Argument" */
    LookaheadDFA {
        prod0: -1,
        transitions: &[
            Trans(0, 5, 4, 29),
            Trans(0, 7, 3, 28),
            Trans(0, 8, 2, 27),
            Trans(0, 13, 1, 26),
        ],
        k: 1,
    },
    /* 1 - "Command" */
    LookaheadDFA {
        prod0: 31,
        transitions: &[],
        k: 0,
    },
    /* 2 - "CommandLine" */
    LookaheadDFA {
        prod0: 34,
        transitions: &[],
        k: 0,
    },
    /* 3 - "CommandList" */
    LookaheadDFA {
        prod0: -1,
        transitions: &[
            Trans(0, 5, 1, 32),
            Trans(0, 6, 2, 33),
            Trans(0, 7, 1, 32),
            Trans(0, 8, 1, 32),
            Trans(0, 10, 2, 33),
            Trans(0, 12, 2, 33),
            Trans(0, 13, 1, 32),
        ],
        k: 1,
    },
    /* 4 - "CommandReplacement" */
    LookaheadDFA {
        prod0: 30,
        transitions: &[],
        k: 0,
    },
    /* 5 - "Source" */
    LookaheadDFA {
        prod0: 35,
        transitions: &[],
        k: 0,
    },
    /* 6 - "SourceList" */
    LookaheadDFA {
        prod0: -1,
        transitions: &[
            Trans(0, 0, 2, 40),
            Trans(0, 9, 1, 36),
            Trans(0, 10, 1, 36),
            Trans(0, 12, 1, 36),
            Trans(0, 13, 1, 36),
        ],
        k: 1,
    },
    /* 7 - "SourceListGroup" */
    LookaheadDFA {
        prod0: -1,
        transitions: &[
            Trans(0, 9, 3, 39),
            Trans(0, 10, 2, 38),
            Trans(0, 12, 2, 38),
            Trans(0, 13, 1, 37),
        ],
        k: 1,
    },
    /* 8 - "TermBackslashLineBreak" */
    LookaheadDFA {
        prod0: 6,
        transitions: &[],
        k: 0,
    },
    /* 9 - "TermBraceGroup" */
    LookaheadDFA {
        prod0: 2,
        transitions: &[],
        k: 0,
    },
    /* 10 - "TermComment" */
    LookaheadDFA {
        prod0: 4,
        transitions: &[],
        k: 0,
    },
    /* 11 - "TermLBracket" */
    LookaheadDFA {
        prod0: 0,
        transitions: &[],
        k: 0,
    },
    /* 12 - "TermLineBreak" */
    LookaheadDFA {
        prod0: 7,
        transitions: &[],
        k: 0,
    },
    /* 13 - "TermRBracket" */
    LookaheadDFA {
        prod0: 1,
        transitions: &[],
        k: 0,
    },
    /* 14 - "TermSemiColon" */
    LookaheadDFA {
        prod0: 5,
        transitions: &[],
        k: 0,
    },
    /* 15 - "TermStringGroup" */
    LookaheadDFA {
        prod0: 3,
        transitions: &[],
        k: 0,
    },
    /* 16 - "TermWord" */
    LookaheadDFA {
        prod0: 8,
        transitions: &[],
        k: 0,
    },
    /* 17 - "TokenBraceGroup" */
    LookaheadDFA {
        prod0: 11,
        transitions: &[],
        k: 0,
    },
    /* 18 - "TokenBraceGroupOpt" */
    LookaheadDFA {
        prod0: -1,
        transitions: &[
            Trans(0, 5, 2, 13),
            Trans(0, 6, 2, 13),
            Trans(0, 7, 2, 13),
            Trans(0, 8, 2, 13),
            Trans(0, 10, 2, 13),
            Trans(0, 11, 1, 12),
            Trans(0, 12, 2, 13),
            Trans(0, 13, 2, 13),
        ],
        k: 1,
    },
    /* 19 - "TokenEnd" */
    LookaheadDFA {
        prod0: -1,
        transitions: &[Trans(0, 10, 2, 10), Trans(0, 12, 1, 9)],
        k: 1,
    },
    /* 20 - "TokenLBracket" */
    LookaheadDFA {
        prod0: 17,
        transitions: &[],
        k: 0,
    },
    /* 21 - "TokenLBracketOpt" */
    LookaheadDFA {
        prod0: -1,
        transitions: &[Trans(0, 11, 1, 18), Trans(0, 13, 2, 19)],
        k: 1,
    },
    /* 22 - "TokenRBracket" */
    LookaheadDFA {
        prod0: 20,
        transitions: &[],
        k: 0,
    },
    /* 23 - "TokenRBracketOpt" */
    LookaheadDFA {
        prod0: -1,
        transitions: &[
            Trans(0, 5, 2, 22),
            Trans(0, 6, 2, 22),
            Trans(0, 7, 2, 22),
            Trans(0, 8, 2, 22),
            Trans(0, 10, 2, 22),
            Trans(0, 11, 1, 21),
            Trans(0, 12, 2, 22),
            Trans(0, 13, 2, 22),
        ],
        k: 1,
    },
    /* 24 - "TokenStringGroup" */
    LookaheadDFA {
        prod0: 14,
        transitions: &[],
        k: 0,
    },
    /* 25 - "TokenStringGroupOpt" */
    LookaheadDFA {
        prod0: -1,
        transitions: &[
            Trans(0, 5, 2, 16),
            Trans(0, 6, 2, 16),
            Trans(0, 7, 2, 16),
            Trans(0, 8, 2, 16),
            Trans(0, 10, 2, 16),
            Trans(0, 11, 1, 15),
            Trans(0, 12, 2, 16),
            Trans(0, 13, 2, 16),
        ],
        k: 1,
    },
    /* 26 - "TokenWord" */
    LookaheadDFA {
        prod0: 23,
        transitions: &[],
        k: 0,
    },
    /* 27 - "TokenWordOpt" */
    LookaheadDFA {
        prod0: -1,
        transitions: &[
            Trans(0, 5, 2, 25),
            Trans(0, 6, 2, 25),
            Trans(0, 7, 2, 25),
            Trans(0, 8, 2, 25),
            Trans(0, 10, 2, 25),
            Trans(0, 11, 1, 24),
            Trans(0, 12, 2, 25),
            Trans(0, 13, 2, 25),
        ],
        k: 1,
    },
];

pub const PRODUCTIONS: &[Production; 41] = &[
    // 0 - TermLBracket: '[';
    Production {
        lhs: 11,
        production: &[ParseType::T(5)],
    },
    // 1 - TermRBracket: ']';
    Production {
        lhs: 13,
        production: &[ParseType::T(6)],
    },
    // 2 - TermBraceGroup: /\{[^}]*\}/;
    Production {
        lhs: 9,
        production: &[ParseType::T(7)],
    },
    // 3 - TermStringGroup: "\u{0022}(?:\\[\u{0022}\\/bfnrt]|u[0-9a-fA-F]{4}|[^\u{0022}\\])*\u{0022}";
    Production {
        lhs: 15,
        production: &[ParseType::T(8)],
    },
    // 4 - TermComment: /#.*(\r\n|\r|\n|$)/;
    Production {
        lhs: 10,
        production: &[ParseType::T(9)],
    },
    // 5 - TermSemiColon: ';';
    Production {
        lhs: 14,
        production: &[ParseType::T(10)],
    },
    // 6 - TermBackslashLineBreak: /\\(\r\n|\r|\n)/;
    Production {
        lhs: 8,
        production: &[ParseType::T(11)],
    },
    // 7 - TermLineBreak: /(\r\n|\r|\n|$)/;
    Production {
        lhs: 12,
        production: &[ParseType::T(12)],
    },
    // 8 - TermWord: /[^\s\[\]]+/;
    Production {
        lhs: 16,
        production: &[ParseType::T(13)],
    },
    // 9 - TokenEnd: TermLineBreak;
    Production {
        lhs: 19,
        production: &[ParseType::N(12)],
    },
    // 10 - TokenEnd: TermSemiColon;
    Production {
        lhs: 19,
        production: &[ParseType::N(14)],
    },
    // 11 - TokenBraceGroup: TermBraceGroup TokenBraceGroupOpt /* Option */;
    Production {
        lhs: 17,
        production: &[ParseType::N(18), ParseType::N(9)],
    },
    // 12 - TokenBraceGroupOpt: TermBackslashLineBreak;
    Production {
        lhs: 18,
        production: &[ParseType::N(8)],
    },
    // 13 - TokenBraceGroupOpt: ;
    Production {
        lhs: 18,
        production: &[],
    },
    // 14 - TokenStringGroup: TermStringGroup TokenStringGroupOpt /* Option */;
    Production {
        lhs: 24,
        production: &[ParseType::N(25), ParseType::N(15)],
    },
    // 15 - TokenStringGroupOpt: TermBackslashLineBreak;
    Production {
        lhs: 25,
        production: &[ParseType::N(8)],
    },
    // 16 - TokenStringGroupOpt: ;
    Production {
        lhs: 25,
        production: &[],
    },
    // 17 - TokenLBracket: TermLBracket TokenLBracketOpt /* Option */;
    Production {
        lhs: 20,
        production: &[ParseType::N(21), ParseType::N(11)],
    },
    // 18 - TokenLBracketOpt: TermBackslashLineBreak;
    Production {
        lhs: 21,
        production: &[ParseType::N(8)],
    },
    // 19 - TokenLBracketOpt: ;
    Production {
        lhs: 21,
        production: &[],
    },
    // 20 - TokenRBracket: TermRBracket TokenRBracketOpt /* Option */;
    Production {
        lhs: 22,
        production: &[ParseType::N(23), ParseType::N(13)],
    },
    // 21 - TokenRBracketOpt: TermBackslashLineBreak;
    Production {
        lhs: 23,
        production: &[ParseType::N(8)],
    },
    // 22 - TokenRBracketOpt: ;
    Production {
        lhs: 23,
        production: &[],
    },
    // 23 - TokenWord: TermWord TokenWordOpt /* Option */;
    Production {
        lhs: 26,
        production: &[ParseType::N(27), ParseType::N(16)],
    },
    // 24 - TokenWordOpt: TermBackslashLineBreak;
    Production {
        lhs: 27,
        production: &[ParseType::N(8)],
    },
    // 25 - TokenWordOpt: ;
    Production {
        lhs: 27,
        production: &[],
    },
    // 26 - Argument: TokenWord;
    Production {
        lhs: 0,
        production: &[ParseType::N(26)],
    },
    // 27 - Argument: TokenStringGroup;
    Production {
        lhs: 0,
        production: &[ParseType::N(24)],
    },
    // 28 - Argument: TokenBraceGroup;
    Production {
        lhs: 0,
        production: &[ParseType::N(17)],
    },
    // 29 - Argument: CommandReplacement;
    Production {
        lhs: 0,
        production: &[ParseType::N(4)],
    },
    // 30 - CommandReplacement: TokenLBracket Command TokenRBracket;
    Production {
        lhs: 4,
        production: &[ParseType::N(22), ParseType::N(1), ParseType::N(20)],
    },
    // 31 - Command: TokenWord CommandList /* Vec */;
    Production {
        lhs: 1,
        production: &[ParseType::N(3), ParseType::N(26)],
    },
    // 32 - CommandList: Argument CommandList;
    Production {
        lhs: 3,
        production: &[ParseType::N(3), ParseType::N(0)],
    },
    // 33 - CommandList: ;
    Production {
        lhs: 3,
        production: &[],
    },
    // 34 - CommandLine: Command TokenEnd;
    Production {
        lhs: 2,
        production: &[ParseType::N(19), ParseType::N(1)],
    },
    // 35 - Source: SourceList /* Vec */;
    Production {
        lhs: 5,
        production: &[ParseType::N(6)],
    },
    // 36 - SourceList: SourceListGroup SourceList;
    Production {
        lhs: 6,
        production: &[ParseType::N(6), ParseType::N(7)],
    },
    // 37 - SourceListGroup: CommandLine;
    Production {
        lhs: 7,
        production: &[ParseType::N(2)],
    },
    // 38 - SourceListGroup: TokenEnd;
    Production {
        lhs: 7,
        production: &[ParseType::N(19)],
    },
    // 39 - SourceListGroup: TermComment;
    Production {
        lhs: 7,
        production: &[ParseType::N(10)],
    },
    // 40 - SourceList: ;
    Production {
        lhs: 6,
        production: &[],
    },
];

static TOKENIZERS: Lazy<Vec<(&'static str, Tokenizer)>> = Lazy::new(|| {
    vec![(
        "INITIAL",
        Tokenizer::build(TERMINALS, SCANNER_0.0, SCANNER_0.1).unwrap(),
    )]
});

pub fn parse<'t, T>(
    input: &'t str,
    file_name: T,
    user_actions: &mut SdcGrammar<'t>,
) -> Result<ParseTree<'t>, ParolError>
where
    T: AsRef<Path>,
{
    let mut llk_parser = LLKParser::new(
        5,
        LOOKAHEAD_AUTOMATA,
        PRODUCTIONS,
        TERMINAL_NAMES,
        NON_TERMINALS,
    );
    llk_parser.trim_parse_tree();
    // Initialize wrapper
    let mut user_actions = SdcGrammarAuto::new(user_actions);

    llk_parser.parse(
        TokenStream::new(input, file_name, &TOKENIZERS, MAX_K).unwrap(),
        &mut user_actions,
    )
}
